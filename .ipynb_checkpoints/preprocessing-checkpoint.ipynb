{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cee28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947c07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('politifact_full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d904e31-9514-4b04-bbbc-ce32742f4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e68839d-97df-4a6e-a713-e700fdedf272",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['ruling'] = dff['ruling'].astype(str).str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6a94cf-1a6b-4a2d-b8ec-f9b02841da1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ruling\n",
       "false          1039\n",
       "pants-fire      238\n",
       "barely-true      96\n",
       "half-true        56\n",
       "mostly-true      40\n",
       "true             24\n",
       "full-flop         4\n",
       "half-flip         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff['ruling'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cff2b2-6d3a-4f71-b219-98d18d2f387b",
   "metadata": {},
   "source": [
    "#### mapping ruling to 3 labels 'supports', 'refutes', and 'uncertain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0252ce9c-8e67-438a-be67-b35070fae0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping ruling to 3 labels supports, refutes, and uncertain\n",
    "def map_ruling_to_label(r):\n",
    "    if r in ['true', 'mostly-true', 'half-true']:\n",
    "        return 'supports'\n",
    "\n",
    "    elif r in ['false', 'pants-fire', 'full-flop', 'half-flip']:\n",
    "        return 'refutes'\n",
    "\n",
    "    elif r in ['barely_true']:\n",
    "        return 'uncertain'\n",
    "\n",
    "    else: \n",
    "        return 'uncertain'\n",
    "\n",
    "# dff['ruling'] = dff['ruling'].apply(lambda r: map_ruling_to_label(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8d1ff7-c801-4e43-8954-d17307b63e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['label_multi'] = dff['ruling'].apply(lambda r: map_ruling_to_label(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39333bed-a565-4dd6-9cfd-8551b840d0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_multi\n",
       "refutes      1283\n",
       "supports      120\n",
       "uncertain      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff['label_multi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77281b7d-7ad1-48c3-b027-1aa8acb94da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['label_bin'] = dff['label_multi'].map({'supports': 1, 'refutes': 0, 'uncertain': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb6e14-3c35-4e24-b12f-5962fec08807",
   "metadata": {},
   "source": [
    "#### Cleaning text fields present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a06138-7e57-4504-bfc2-19b654f938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b305ba22-de84-4c72-b867-01584445b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['claim', 'desc', 'tags', 'summary', 'article', 'speaker']:\n",
    "    dff[col] = dff[col].fillna('').astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30861464-8f06-46cc-a64b-79cab0a003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing url's in the article\n",
    "dff['article_nourl'] = dff['article'].str.replace(r'http\\S+', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa5ad7-a871-44ab-a3e6-ceefc6027464",
   "metadata": {},
   "source": [
    "#### Changing dates to a datetime format and handling exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37bce593-e102-4556-bebe-7a9f061c9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def parse_date(s):\n",
    "    try:\n",
    "        return parser.parse(s)\n",
    "    except Exception:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cb577fc-cdea-496f-af16-3ad6f92b49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = dff['date'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18b462e1-7329-44f6-ab0b-b330ef3a4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_date_indexes = test_series[test_series.isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e51005b2-6fd9-4524-941a-7721cf257fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      Septiembre 5, 2025\n",
       "24        Agosto 15, 2025\n",
       "29         Agosto 7, 2025\n",
       "55         Julio 16, 2025\n",
       "67          Julio 7, 2025\n",
       "68          Julio 3, 2025\n",
       "69          Julio 2, 2025\n",
       "115         Mayo 28, 2025\n",
       "123         Mayo 21, 2025\n",
       "124         Mayo 21, 2025\n",
       "133         Mayo 16, 2025\n",
       "141          Mayo 9, 2025\n",
       "170        Abril 11, 2025\n",
       "184         Abril 4, 2025\n",
       "194         Abril 3, 2025\n",
       "216        Marzo 27, 2025\n",
       "217        Marzo 27, 2025\n",
       "221        Marzo 25, 2025\n",
       "222        Marzo 25, 2025\n",
       "226        Marzo 21, 2025\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.loc[null_date_indexes, 'date'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55e644d9-1682-4c7a-a7dd-61b8cf048a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above experiment we observe that in some cases, the names of month written in Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6afb7861-b59e-41a0-87fc-af89954aeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_to_english = {\n",
    "    r'\\bEnero\\b': 'January', r'\\benero\\b': 'January',\n",
    "    r'\\bFebrero\\b': 'February', r'\\bfebrero\\b': 'February',\n",
    "    r'\\bMarzo\\b': 'March', r'\\bmarzo\\b': 'March',\n",
    "    r'\\bAbril\\b': 'April', r'\\babril\\b': 'April',\n",
    "    r'\\bMayo\\b': 'May', r'\\bmayo\\b': 'May',\n",
    "    r'\\bJunio\\b': 'June', r'\\bjunio\\b': 'June',\n",
    "    r'\\bJulio\\b': 'July', r'\\bjulio\\b': 'July',\n",
    "    r'\\bAgosto\\b': 'August', r'\\bagosto\\b': 'August',\n",
    "    r'\\bSeptiembre\\b': 'September', r'\\bseptiembre\\b': 'September',\n",
    "    r'\\bOctubre\\b': 'October', r'\\boctubre\\b': 'October',\n",
    "    r'\\bNoviembre\\b': 'November', r'\\bnoviembre\\b': 'November',\n",
    "    r'\\bDiciembre\\b': 'December', r'\\bdiciembre\\b': 'December'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f3292ac-b77d-4703-81a5-224ad2037c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_months(series):\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    s = s.str.strip()\n",
    "    for pattern, repl in spanish_to_english.items():\n",
    "        s = s.str.replace(pattern, repl, regex = True)\n",
    "    s = s.replace('', pd.NA)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dba5f832-8918-4b14-bddb-6f39e451b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['date_norm'] = replace_months(dff['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57e582a3-48e5-4bb3-aac0-ad34080444e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['date_parsed'] = dff['date_norm'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3e4759a-300e-43aa-ba7b-f0c3fcd704b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing dates\n"
     ]
    }
   ],
   "source": [
    "print(dff['date_parsed'].isna().sum(), 'missing dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e408ad65-10bf-45bb-90e0-3a0fc5047fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so by converting spanish month names to english using regex replacement, we were able to achieve 100% accuracy in date conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e02adc-ba92-484f-b4cb-dba6c6e0484f",
   "metadata": {},
   "source": [
    "#### Creating claim and article lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d5943a76-d6da-43f7-a77d-1a5a9d7972ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['claim_len'] = dff['claim'].str.split().apply(len)\n",
    "dff['article_len'] = dff['article'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8f22b-4ee2-418a-9fc6-976095556cd0",
   "metadata": {},
   "source": [
    "#### Making evidence snippets of few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46535935-18d6-41c0-b3c1-3520d6cfa375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_snippet(row, n_words = 80):\n",
    "    if row['summary']:\n",
    "        return row['summary'][:2000]\n",
    "    art = row['article_nourl'] if 'article_nourl' in row else row['article']\n",
    "    words = art.split()\n",
    "    return \" \".join(words[:n_words])\n",
    "\n",
    "dff['evidence_snippet'] = dff.apply(make_snippet, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c4b05a8-9d69-4be7-a63c-f3ace6a128dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_keywords = ['facebook','instagram','x posts','twitter','tik tok','tiktok','threads','social media','viral']\n",
    "dff['is_social'] = dff['speaker'].str.lower().apply(lambda s: any(k in s for k in social_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93858b7-4192-4eae-8419-9a3ca10bcd69",
   "metadata": {},
   "source": [
    "#### dropping duplicates for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9333f9c4-51f9-43aa-8cd1-85488d415d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.drop_duplicates(subset = ['url'])\n",
    "dff = dff.drop_duplicates(subset = ['claim', 'article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "feabd8b7-8d3f-4c30-ae37-a500f7058938",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv('politifact_preprocessed_step1.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3ec03-be16-4951-a2b3-50bbb76d5eb7",
   "metadata": {},
   "source": [
    "#### making a claims dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2357300b-5d4a-4250-bbce-1cbfda91b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims dataset: (1403, 5)\n"
     ]
    }
   ],
   "source": [
    "claims_df = dff[dff['label_multi'].isin(['supports', 'refutes'])].copy()\n",
    "claim_cols = ['claim', 'label_multi', 'label_bin', 'speaker', 'date_parsed']\n",
    "claims_df = claims_df[claim_cols]\n",
    "claims_df.to_csv('claims_dataset.csv', index = False)\n",
    "print('Claims dataset:', claims_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958406d2-d30a-4167-a59b-57a33ab7377d",
   "metadata": {},
   "source": [
    "#### chunking large articles into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10b1aaa-3c1b-40b7-90fa-8bf6bf4efeb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m evidence_rows = []\n\u001b[32m     10\u001b[39m chunk_id = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m indx, row \u001b[38;5;129;01min\u001b[39;00m dff.iterrows():\n\u001b[32m     12\u001b[39m     art = row[\u001b[33m'\u001b[39m\u001b[33marticle_nourl\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33marticle_nourl\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m art.strip():\n",
      "\u001b[31mNameError\u001b[39m: name 'dff' is not defined"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size = 250):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "evidence_rows = []\n",
    "chunk_id = 0\n",
    "for indx, row in dff.iterrows():\n",
    "    art = row['article_nourl'] if 'article_nourl' in row else row['article']\n",
    "    if not art.strip():\n",
    "        continue\n",
    "    chunks = chunk_text(art, chunk_size = 250)\n",
    "    for c in chunks:\n",
    "        evidence_rows.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"url\": row['url'],\n",
    "            \"claim\": row['claim'],\n",
    "            \"ruling\": row['ruling'],\n",
    "            \"label_multi\": row['label_multi'],\n",
    "            \"speaker\": row['speaker'],\n",
    "            \"date\": row['date_parsed'],\n",
    "            \"chunk_text\": c\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "evidence_df = pd.DataFrame(evidence_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "900ac2ea-93bd-499c-a022-16c6659b55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180048, 8)\n"
     ]
    }
   ],
   "source": [
    "print(evidence_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c32ad36-251e-4a90-aef1-43687f4d91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_df.to_parquet('evidence_chunks.parquet', index = False, engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10bf72-145f-4a6c-8e4d-69cbf5ca5404",
   "metadata": {},
   "source": [
    "#### making a claim-evidence pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1041182d-9208-48db-840a-4bc3aea05eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for indx, row in dff.iterrows():\n",
    "    pairs.append({\n",
    "        'claim': row['claim'],\n",
    "        'evidence': row['evidence_snippet'],\n",
    "        'label': row['label_multi']\n",
    "    })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "pairs_df.to_csv('claim_evidence_pairs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da791ca-8f92-4ce5-98b0-01f14888ee14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
